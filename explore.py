import re
import time
from datetime import datetime
import numpy as np
import pandas as pd
from pandas.io.json import json_normalize
from sklearn.preprocessing import Imputer
import category_encoders as ce
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelBinarizer
from sklearn.feature_selection import VarianceThreshold
from sklearn.externals import joblib
import multiprocessing
from threading import Thread

num_cores = multiprocessing.cpu_count()
print('total cores: {}'.format(num_cores))

def get_categorical_cols(df):
    dt_df = df.dtypes
    col_idx = [k for k,t in enumerate(dt_df) if t=='object']
    #cat_data = df.select_dtypes(include=['object']).copy()
    cat_cols = [c for c in df if c.endswith('Identifier') or 
                                 c.endswith('Enabled') or 
                                 c.endswith('Capable') or
                                 c.startswith('Is') or
                                 c.startswith('Has') or
                                 c.startswith('Census_Is') or
                                 c.startswith('Census_Has')
                                 ]
    cat_cols = cat_cols + ['Wdft_IsGamer','SMode', 'Firewall', 'AutoSampleOptIn', 'AVProductsInstalled', 'UacLuaenable']
    cat_set = set(cat_cols + list(dt_df[col_idx].index))
    return list(cat_set)

def to_lower(data):
    dt_df = data.dtypes
    col_idx = [k for k,t in enumerate(dt_df) if t=='object']
    cols = data.columns[col_idx]
    for c in cols:
        data[c] = data[c].str.lower()
        
    return data    

load_raw = 0

def load_h5():
    train_raw = pd.read_hdf('input/train_raw.h5', 'train_raw')
    train_raw['DisplayHorzVertRatio'] = train_raw.Census_InternalPrimaryDisplayResolutionHorizontal/train_raw.Census_InternalPrimaryDisplayResolutionVertical
        
    data_raw = pd.read_hdf('input/data_raw.h5', 'data_raw')
    data_raw['DisplayHorzVertRatio'] = data_raw.Census_InternalPrimaryDisplayResolutionHorizontal/data_raw.Census_InternalPrimaryDisplayResolutionVertical
    # drop columns 
    cols = ['DefaultBrowsersIdentifier','Census_InternalPrimaryDisplayResolutionHorizontal',
               'Census_InternalPrimaryDisplayResolutionVertical']

    #drop following columns because of the too many NAN
    cols = cols + ['Census_IsWIMBootEnabled','Census_ThresholdOptIn','Census_InternalBatteryType',
          'Census_IsFlightingInternal','Census_ProcessorClass','PuaMode']
    data_raw.drop(cols, axis=1, inplace=True)
    
    labels = pd.read_hdf('input/labels.h5', 'labels')
    
    return train_raw, labels, data_raw
    #return train_raw, labels, data_raw

    
def load_cat_non_cat_data():
    cat_data = pd.read_pickle('input/cat_data.pkl')
    non_cat_data = pd.read_pickle('input/non_cat_data.pkl')
    for c in cat_data.columns:
        cat_data[c] = cat_data[c].astype('category')
    return cat_data, non_cat_data

def dummy(col_names=[]):
    if (len(col_names)>1):
        for c in col_names:
            print(c)
    else:
        print(col_names)
        
def hash_encode(df,col_names=[], 
                label_col='HasDetections', 
                accumulate=True, 
                return_df=True, 
                dynamic_n_component=False, 
                drop_invariant=True, 
                debug=True, 
                save_intermediate_results=False):
    col_names=list(col_names)
    if len(col_names)==0:
        col_names = list(df.columns)
    
    if label_col in col_names:
        col_names.remove(label_col)
        
    print('hash encoding: {}'.format(col_names), flush=True)    
    _frames = []
    _cols_arr = []
    for c in col_names:
        if debug:
            print('processing: {}'.format(c), flush=True)
            
        _model = df[[c]]
        
        _val_ct = df[c].value_counts().count()
        
        _n_comp = 8* int(np.log(_val_ct)) if dynamic_n_component and _val_ct > 10 else 8
                    
        hash_enc = ce.HashingEncoder(cols=[c], return_df=return_df, n_components=_n_comp, drop_invariant=drop_invariant)
        print('hash_enc: {}'.format(hash_enc), flush=True)
        hash_enc.fit(_model, df[label_col])
        
        _df = hash_enc.transform(_model)
        
        if return_df:
            _df.columns = '{}_'.format(c)+ _df.columns
        else:    
            _cols_arr.append(c) 
        
        if accumulate:    
            _frames.append(_df)
            
        if save_intermediate_results:
            _dict = {'col': c, 'hash_enc': hash_enc, 'data': _df}
            _fl_name = 'input/{}_df_train.pkl'.format(c) if return_df else 'input/{}_np_train.pkl'.format(c)
            joblib.dump(_dict, _fl_name)
            
    return pd.concat(_frames, axis=1) if return_df else (_frames, _cols_arr)

ignore_cols = ['IeVerIdentifier','Census_IsFlightsDisabled',
             'Census_HasOpticalDiskDrive','Census_OSInstallLanguageIdentifier',
             'LocaleEnglishNameIdentifier','Census_IsPortableOperatingSystem',
             'Census_IsTouchEnabled','Census_OSUILocaleIdentifier',
             'Census_IsPenCapable','Census_DeviceFamily','Census_PowerPlatformRoleName',
             'Census_OSBranch','OrganizationIdentifier','Census_IsVirtualDevice',
             'AVProductsInstalled','Census_IsSecureBootEnabled','IsBeta',
             'IsSxsPassiveMode','Census_IsAlwaysOnAlwaysConnectedCapable','IsProtected']
ignore_cols = list(set(ignore_cols))

print('loading categorical and numerical data...', flush=True)
st = time.time()
cat_data = pd.read_pickle('input/cat_data.pkl')

df = cat_data.drop(ignore_cols,axis=1)
col_names_arr = np.array_split(list(df.columns), num_cores)

print('completed loading categorical and numerical data: {} sec'.format(time.time() - st), flush=True)

print('hash encoding categorical data...', flush=True)

#col_names_arr = np.array_split(ignore_cols, num_cores)

st = time.time()
th_arr = []
for n in range(2):
    args = {'col_names':col_names_arr[n], 'label_col':'HasDetections', 
        'accumulate':False, 
        'return_df':False, 
        'dynamic_n_component':True, 
        'drop_invariant':False, 
        'debug':True, 
        'save_intermediate_results':True}
    #args = {'col_names': col_names_arr[n]}

    t = Thread(target=hash_encode, args=(df,), kwargs=args)
    t.start()
    th_arr.append(t)
    
map(lambda t : t.join, th_arr)
    
#cat_data = hash_encode(cat_data, dynamic_n_component=False, debug=True, drop_invariant=False)

print('completed encoding categorical data: {} sec'.format(time.time() - st), flush=True)
#cat_data.to_pickle('input/cat_data_hash_encoded.pkl')
#print('saved encoded categorical data')
